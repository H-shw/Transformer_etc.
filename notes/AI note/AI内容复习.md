# AI内容复习

[toc]

## 数学知识

##### 凸函数 拉格朗日函数 KTT

https://zhuanlan.zhihu.com/p/38163970

## 基本概念与运用

##### 有监督学习和无监督学习

**有监督学习**算法**有训练过程**，算法用训练集进行学习，用学习得到的模型进行预测。通 常所见的机器学习应用，如图像识别、语音识别等都属于有监督学习问题。有监督学习的样本由输入值与标签值组成$(x,y)$ , 目标是找到映射函数 $y=h(x)$ 。

**无监督学**习对**无标签的样本**进行分析，发现样本集的结构或者分布规律。其典型代表是 **聚类**，表示学习，以及数据降维。表示学习也是无监督学习，样本中自动学习出有用的特征，用于分类或聚类任务。

聚类举例：

##### Kmeans

1. 定义 K 个重心。一开始这些重心是随机的（也有一些更加有效的用于初始化重心的算法）
2. 寻找最近的重心并且更新聚类分配。将每个数据点都分配给这 K 个聚类中的一个。每个数据点都被分配给离它们最近的重心的聚类。这里的「接近程度」的度量是一个超参数——通常是欧几里得距离（Euclidean distance）。
3. 将重心移动到它们的聚类的中心。每个聚类的重心的新位置是通过计算该聚类中所有数据点的平均位置得到的。

##### 神经网络中的无监督方法：AutoEcoder

它基于反向传播算法与最优化方法(如梯度下降法)，利用输入数据X本身作为监督,来指导神经网络尝试学习一个映射关系, 从而得到一个重构输出$x^R$。在时间序列异常检测场景下,异常对于正常来说是少数,所以我们认为，如果使用自编码器重构出来的输出$x^R$跟原始输入的差异超出一定阈值(threshold) 的
话，原始时间序列即存在了异常。

通过算法模型包含两个主要的部分: **Encoder** (编码器)和 **Decoder** (解码器)。

编码器的作用是把高维输入 $X$ 编码成低维的隐变量 $h$ 从而强迫神经网络学习最有信息量的特征;

解码器的作用是把隐藏层的隐变量h还原到初始维度,最好的状态就是解码器的输出能够完美地或者近似恢复出原来的输入,即 $x^R≈x$ .

![1.1.png](https://github.com/H-shw/Transformer_etc./blob/master/notes/AI%20note/pics/1.1.png?raw=true)

https://github.com/H-shw/Transformer_etc./blob/master/notes/AI%20note/pics/1.1.png

(1)从输入层->隐藏层的原始数据X的编码过程:
$h= g_{\theta_1}(x)=σ(W1x + b1)$

(2)从隐藏层->输出层的解码过程:

$\hat x= g_{\theta_2}(h)=σ(W_2h+b2)$

那么算法的优化目标函数就写为: $MinimizeLoss = dist(X, X^R)$
其中dist为二者的距离度量函数,通常用 MSE (均方方差)。

$MSE = \frac{1}{n}\sum_{i=1}^{m}w_i{(y_i-\hat{y_i})^{2}}$

自编码可以实现类似于PCA等数据降维、数据压缩的特性。从上面自编码的网络结构图，如果输入层神经元的个数n大于隐层神经元个数m,那么我们就相当于把数据从n维降到了m维;然后我们利用这m维的特征向量,进行重构原始的数据。

这个跟PCA降维一模一样,只不过PCA是通过求解特征向量,进行降维,是一种线性的降维方式，而自编码是一种非线性降维。

##### PCA

主成分分析是把多指标转化为少数几个综合指标。

主成分分析经常用减少数据集的维数，同时保持数据集的对方差贡献最大的特征。这是通过保留低阶主成分，忽略高阶主成分做到的。这样低阶成分往往能够保留住数据的最重要方面。

变换的步骤：

1. 第一步计算矩阵 X 的样本的[协方差矩阵](https://baike.baidu.com/item/协方差矩阵/9822183) S（此为不标准PCA，标准PCA计算[相关系数](https://baike.baidu.com/item/相关系数/3109424)[矩阵](https://easyai.tech/ai-definition/matrix/)C）(协方差的意义是，衡量两个变量偏差变化趋势是否一致，除以两变量标准差之积以标准化，即相关系数。https://blog.csdn.net/qq_23100417/article/details/84935692)
2. 第二步计算协方差矩阵S（或C）的[特征向量](https://baike.baidu.com/item/特征向量/8663983) **e**1,**e**2,…,**e**N和特征值 , t = 1,2,…,N
3. 第三步投影数据到特征向量张成的空间之中。利用下面公式，其中BV值是原样本中对应维度的值。

​	$newBV_{i,p}=\sum_{k=1}^{n} {e_{i}BV_{i,k}}$



##### 奇异值分解

假设*M*是一个*m×n*阶[矩阵](https://zh.wikipedia.org/wiki/矩陣)，其中的元素全部属于[域](https://zh.wikipedia.org/wiki/体_(数学))*K*，也就是[实数](https://zh.wikipedia.org/wiki/實數)域或[复数](https://zh.wikipedia.org/wiki/复数_(数学))域。如此则存在一个分解使得

其中*U*是*m×m*阶[酉矩阵](https://zh.wikipedia.org/wiki/酉矩陣)；Σ是*m×n*阶非负[实数](https://zh.wikipedia.org/wiki/实数)[对角矩阵](https://zh.wikipedia.org/wiki/對角矩陣)；而V\*，即*V*的[共轭转置](https://zh.wikipedia.org/wiki/共轭转置)，是*n×n*阶酉矩阵。这样的分解就称作*M*的**奇异值分解**。Σ 对角线上的元素Σ*i*,*i*即为*M*的**奇异值**。

https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3

##### GAN

生成对抗网络（GAN）。它由两个网络组成：一个生成器和一个鉴别器，分别负责伪造图片和识别真假。

生成器产生图像的目的是诱使鉴别者相信它们是真实的，同时，鉴别者会因为发现假图片而获得奖励



##### 分类问题 与 回归问题

对于有监督学习，如果样本标签是整数则称为分类问题。此时的目标是确定样本的类别， 以整数编号。预测函数是向量到整数的映射。

如果标签值是连续实数则称为回归问题。此时预测函数是向量到实数的映射。

例如根据一个人的学历、工作年限等特征预测其收入，是典型的回归问题，收入是实数 值而不是类别标签。



##### 强化学习

强化学习模拟人的行为，源自于行为主义心理学。

类似于有监督学习，通过对状态-动作-回报值序列进行学习，可以得到一个称为策略函数的模型，用于确定在每种状态下要执行的动作。

训练时，对正确的动作做出奖励，对错误的动作进行惩罚，训练完成之后用得到 的模型进行预测。



##### 生成模型 与 判别模型

假设 $x$ 为特征向量，$y$ 为该样本的标签值。

如果机器学习算法对样本特征向量和标签的 联合概率分布 $p(x,y)$ 建模，则称为生成模型。

如果对条件概率 $p(y|x)$ 进行建模，则称为 判别模型。

不使用概率模型的分类算法也属于判别模型，它直接预测样本的标签值而不关心 样本的概率分布，这种情况的预测函数为 $y = f(x)$ 。

这三种模型也分别被称为生成学习，条件学习，以及判别学习。 

还有另外一种定义标准。生成模型对条件概率 $p(x|y)$ 建模，判别模型对条件概率 $p(y|x) $建模。前者不仅可以通过贝叶斯公式用于分类问题，还可用于根据标签值 y （也称 为隐变量）生成随机的样本数据 x ，而后者则只能用于根据样本特征向量 x 的值判断它的标签值 y 的分类任务。

**生成模型**： 学习时先得到 P(x,y)，继而得到 P(y|x)。预测时应用最大后验概率法（MAP）得到预测类别 y 。

MAP : https://blog.csdn.net/gcheney/article/details/108442861

https://blog.csdn.net/fq_wallow/article/details/104383057

**判别模型**： 直接学习得到P(y|x)，利用MAP得到 y。或者直接学得一个映射函数 y = f(x)。

##### 过拟合

模型在训练集上精度高，但在测试集上精度低。

##### 过拟合的原因有哪些

 引起过拟合的可能原因有：

1. 模型本身过于复杂，拟合了训练样本集中的噪声。此时需要选用更简单的模型，或 者对模型进行裁剪。 
2.  训练样本太少或者缺乏代表性。此时需要增加样本数，或者增加样本的多样性。 
3.  训练样本噪声的干扰，导致模型拟合了这些噪声，这时需要剔除噪声数据或者改用 对噪声不敏感的模型。

##### 正则化方法

- **正则化(Regularization)** 是机器学习中对原始损失函数引入额外信息，以便防止过拟合和提高模型泛化性能的一类方法的统称。也就是目标函数变成了**原始损失函数+额外项**，常用的额外项一般有两种，中文称作**L1正则化**和**L2正则化**，或者L1范数和L2范数（实际是L2范数的平方）。
- L1正则化和L2正则化可以看做是**损失函数的惩罚项**。所谓**惩罚**是指对损失函数中的**某些参数做一些限制**。对于线性回归模型，**使用L1正则化的模型叫做Lasso回归，使用L2正则化的模型叫做Ridge回归（岭回归）**。
- https://www.cnblogs.com/zingp/p/10375691.html

L1 正则化 

L2 正则化 

- L1正则化是指权值向量w中各个元素的绝对值之和，通常表示为∥w∥1
- L2正则化是指权值向量w中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为∥w∥2
- L2防止过拟合的原因：**拟合过程中通常都倾向于让权值尽可能小**，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。**可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响**，专业一点的说法是**抗扰动能力强**。
- $\theta_{j} = \theta_{j}(1-\alpha\frac{\lambda}{n})-\alpha\frac{1}{n}\sum_{i=1}^{n}(h\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}$
- **其中λ就是正则化参数**。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj 都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。
  最开始也提到L1正则化一定程度上也可以防止过拟合。之前做了解释，当L1的正则化系数很小时，得到的最优解会很小，可以达到和L2正则化类似的效果。

决策树的剪枝算法 

神经网络训练中的 dropout 技术

提前终止技术

##### ROC 曲线的原理

 对于二分类问题可以通过调整分类器的灵敏度得到不同的分类结果，从而在二者之间折中。将各种灵敏度下的性能指标连成曲线可以得到 ROC 曲线，它能够更全面的反映算法的性能。 

真阳率（TPR）即召回率，是正样本被分类器判定为正样本的比例。

$TPR = TP /(TP + FN)$

在目标检测任务中正样本是要检测的目标，真阳率即检测率，即目标能够被检测出来的 比例。

假阳率（FPR）是负样本被分类器判定为正样本的比例

$FPR = FP /(FP + TN)$ 

对于目标检测问题假阳率即误报率。 

ROC 曲线的横轴为假阳率，纵轴为真阳率。当假阳率增加时真阳率会增加，它是一条增长的曲线

##### AUC

ROC曲线下方的面积（英语：Area under the Curve of ROC (AUC ROC)），其意义是：

- 因为是在1x1的方格里求面积，AUC必在0~1之间。
- 假设阈值以上是阳性，以下是阴性；
- 若随机抽取一个阳性样本和一个阴性样本，分类器**正确判断**阳性样本的值高于阴性样本之**概率**。
- 简单说：**AUC值越大的分类器，正确率越高。**

从AUC判断分类器（预测模型）优劣的标准：

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设置阈值的话，能有预测价值。
- AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

![1.2.png](https://github.com/H-shw/Transformer_etc./blob/master/notes/AI%20note/pics/1.2.png?raw=true)

##### 精度，召回率，F1 值

精度：$P = \frac{TP}{TP+FP}$

召回率：$R=\frac{TP}{TP+FN}$

同样的 R <= 1。精度值越接近 1，对正样本的分类越准确，即查的越准。召回率越接近于1，正样本被正确分类的比例越大，即查的越全。 根据精度和召回率，F1 值定义为

$F1 = \frac{2PR}{P+R}$

是精度和召回率的调和平均数的倒数

$\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})$

##### 交叉验证的原理

 交叉验证用于统计模型的精度值。k 折交叉验证将样本随机、均匀地分为 k 份，轮流用 其中的 k -1份训练模型，1 份用于测试模型的准确率，用 k 个准确率的均值作为最终的准确率。

##### 过拟合 是欠拟合

欠拟合也称为欠学习，指模型在训练集上的精度差。导致欠拟合的常见原因有模型简单， 特征数太少无法正确的建立映射关系。 

过拟合也称为过学习，指模型在训练集上表精度高，但在测试集上精度低，泛化性能差。 

过拟合产生的根本原因是训练数据包含抽样误差，算法训练时模型拟合了抽样误差。所谓抽样误差，是指抽样得到的样本集和整体数据集之间的偏差。

##### 没有免费午餐定理

没有任何一个机器学习模型在所有样本集上表现是最优的。如果在一个数据集上算法 A 优于算法 B，则一定存在另外一个数据集，使得 B 优于 A

##### 奥卡姆剃刀原理

简单的模型通常具有更好的泛化性能

##### 神经网络训练时的目标函数是否为凸函数

一般情况下不是凸函数。因此面临局部极小值和鞍点问题。

**凸问题的局部最优解就是全局最优解**。

##### 凸函数与凸优化

[理解凸优化 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37108430)

##### 混淆矩阵的概念

对于 k 分类问题，混淆矩阵为 k * k 的矩阵，它的元素 $c_{ij}$ 表示第i 类样本被分类器判定 为第 j 类的数量。

对角线上即为划分正确的元素。

##### 贝叶斯 先验概率 后验概率

如果事件 A 是因，事件 B 是果，则称 p(A) 为 先验概率，意为事先已经知道其值。 

p (A|B) 称为后验概率，意为事后才知道其值。

条件概率 p(B|A) 则称为似然函数。

先验概率是根据以往经验和分析得到的概率，在随机事件发生之前即已经知道，是“原因”发生的概率。后验概率是根据“结果”信息所计算出的导致该结果的原因所出现的概率。

后验概率用于在事情已经发生的条件下，分析使得这件事情发生的原因概率。

##### 贝叶斯 拉普拉斯光滑

在类条件概率的计算公式中，如果 , $N_{x_{i}=v,y=c}$ 为 0，即特征分量的某个取值在某一类在训 练样本中一次都不出现，则会导致如果预测样本的特征分量取到这个值时**整个分类判别函数 的值为 0**。

作为补救措施可以使用拉普拉斯平滑，具体做法是给分子和分母同时加上一个正 数。如果特征分量的取值有 k 种情况，将分母加上 k ，每一类的分子加上 1，这样可以保证 所有类的条件概率加起来还是 1



##### 决策树

[决策树(Decision Tree)：通俗易懂之介绍 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/30059442)

CART 指标

Gini 不纯度。样本集的 Gini 不纯度定义：$G(D)=1-\sum_{i}p_{i}^{2}$

C4.5

$GR = \frac{G}{IV}$

其中 $IV=-\sum_{i=1}^{m}\frac{|D_i|}{|D|}ln\frac{|D_i|}{|D|}$

对于分类问题，叶子节点的值如何设定？对于回归问题，决策树叶子节点的值如何设定？

对于分类树，将叶子节点的值设置成本节点的训练样本集中出现概率最大的那个类。即

$y^{*}=arg\,\,max_{y}p_{y}$

对于回归树，则设置为本节点训练样本标签值的均值

$y^{*}=\frac{\sum_{i=1}^{N}y_{i}}{N}$



##### 决策树 预剪枝 后剪枝

决策树的剪枝算法可以分为两类，分别称为预剪枝和后剪枝。前者在树的训练过程中通 过停止分裂对树的规模进行限制；后者先构造出一棵完整的树，然后通过某种规则消除掉部 分节点，用叶子节点替代。

##### 决策树 属性缺失

在某些情况下样本特征向量中一些分量没有值，这称为属性缺失

#####  k 的取值对 k 近邻算法的影响。

如果其值太小，则容易受到噪声的影响，导致泛函性能下降，出现过拟合。

如果 k 值等 于训练样数，则对于任意的预测样本，都会将其预测为训练样本集中数量最大的类

##### KNN用于

k 近邻算法也可以用于回归问题。假设离测试样本最近的 k 个训练样本的标签值为 i y ， 则对样本的回归预测输出值为
$$
\hat{y} = (\sum_{i=1}^{k}y_{i})/k
$$

##### 各种距离

<img src="C:\Users\14163\AppData\Roaming\Typora\typora-user-images\image-20220603200815494.png" alt="image-20220603200815494" style="zoom: 67%;" />

##### LMNN 算法

LMNN 寻找一个变换矩阵，使得变换后每个样本的个最近邻居都和它是同一个类，而不 同类型的样本通过一个大的间隔被分开。

参考习题答案与[大间隔最近邻居_百度百科 (baidu.com)](https://baike.baidu.com/item/大间隔最近邻居/)

##### 数据降维

使用数据降维算法的目的是什么？ 数据降维算法的目标是将向量变换到低维空间中，并保持原始空间中的某些结构信息， 以达到某种目的，如避免维数灾难，数据可视化。

##### 解释 PCA 降维算法的流程

计算投影矩阵的流程： 

1.计算样本集的均值向量。将所有向量减去均值，这称为白化。 

2.计算样本集的协方差矩阵。 

3.对方差矩阵进行特征值分解，得到所有特征值与特征向量。 

4.将特征值从大到小排序，保留最大的一部分特征值对应的特征向量，以它们为行，形成投影矩阵。 

投影算法的流程： 

1.将样本减掉均值向量。 

2.左乘投影矩阵，得到降维后的向量

 PCA 重构算法的流程。

#####  向量重构的流程为：

1.输入向量左乘投影矩阵的转置矩阵。 

2.加上均值向量，得到重构后的结果。

##### 神经网络 激活函数

保证神经网络的映射是非线性的，如果不使用激活函数，无论神经网络有多少层，其所 表示的复合函数还是一个线性函数。

激活函数需要满足： 

1.非线性。保证神经网络实现的映射是非线性的。 

2.几乎处处可导。保证可以用梯度下降法等基于导数的算法进行训练。 

3.单调递增或者递减。保证满足万能逼近定理的要求，且目标函数有较好的特性。

##### sigmoid

$f(x)=\frac{1}{1+e^{-x}}$

$f^{'}(x)=f(x)(1-f(x))$

##### 神经网络参数的初始值如何设定？ 

一般用随机数进行初始化 或者 预训练

**Xavier初始值（sigmod、tanh等S型曲线的权重初始值）**

Xavier等人发现，为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。推导出的结论是，如果前一层的节点数为n，则初始值使用标准差为 $\frac{1}{\sqrt{n}}$ 的分布。

**He初始值（ReLU的权重初始值）**

当前一层的节点数为n时，He初始值使用标准差为 $\sqrt{\frac{2}{n}}$ 的高斯分布。

##### 梯度消失

什么是梯度消失问题，为什么会出现梯度消失问题？ 在用反向传播算法计算误差项时每一层都要乘以本层激活函数的导数。

$\delta^{l}=(W^{l+1})^{T}\delta^{(l+1)}\odot f'(u^{(l)})$

如果激活函数导数的绝对值小于 1，多次连乘之后误差项很快会衰减到接近于 0，参数 的梯度值由误差项计算得到，从而导致前面层的权重梯度接近于 0，参数无法有效的更新， 称为梯度消失问题.

##### 动量项的原理

动量项累积了之前的权重更新值，加上此项之后的参数更新公式为 

$W_{t+1} = W_{t} + V_{t+1} $

其中$V_{t+1} $是动量项，计算公式为

$V_{t+1}=-\alpha\triangledown _{W}L(W_t)+\mu V_t$

它是上一时刻的动量项与本次梯度值的加权平均值，其中 $\alpha$ 是学习率， $\mu$ 是动量项 数。如果按照时间 t 进行展开，则第 t 次迭代时使用了从 1 到t 次迭代时的所有梯度值，且老 的梯度值按照 $\mu^{t}$ 的系数指数级衰减。动量项是为了加快梯度下降法的收敛，它使用历史信息对当前梯度值进行修正，以抵消在病态条件问题上的来回震荡。

## 机器学习基本模型



